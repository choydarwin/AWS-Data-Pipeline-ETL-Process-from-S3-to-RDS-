# AWS Data Pipeline: ETL Process from S3 to RDS  

## ðŸš€ Overview  
This project demonstrates an **ETL (Extract, Transform, Load) pipeline** using **AWS S3 and RDS** to move and process structured data. It focuses on extracting raw data from S3, transforming it using Python, and storing it in a relational database (RDS) for further analysis.  

## ðŸ”¹ Technologies Used  
- **AWS S3** â€“ Cloud storage for raw data  
- **AWS RDS (MySQL/PostgreSQL)** â€“ Relational database for structured data storage  
- **Python & SQL** â€“ Data transformation and loading  
- **Pandas** â€“ Data manipulation and preprocessing  
- **Boto3** â€“ AWS SDK for interacting with S3 and RDS  

## ðŸ”„ ETL Workflow  
1. **Extract** â€“ Retrieve raw data from AWS S3  
2. **Transform** â€“ Clean and format data using Python and Pandas  
3. **Load** â€“ Insert processed data into AWS RDS  

