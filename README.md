# **AWS Data Pipeline: ETL Process from S3 to RDS**

## ğŸš€ Overview  
This project demonstrates an ETL (Extract, Transform, Load) pipeline using AWS S3 and RDS. It focuses on extracting raw data from S3, transforming it using Python, and loading it into a relational database (RDS) for further analysis. The project is designed to showcase my skills in cloud-based data processing and SQL.  

**Note:** The dataset used in this project is sourced from a Coursera project.  

## ğŸ”¹ Technologies Used  
- **AWS S3** â€“ Cloud storage for raw data  
- **AWS RDS (MySQL)** â€“ Relational database for structured data storage  
- **Python** â€“ Data transformation and preprocessing  
- **Pandas** â€“ Data manipulation and cleaning  
- **Boto3** â€“ AWS SDK for interacting with S3 and RDS  
- **SQL** â€“ Database management and querying  
- **MySQL Workbench** â€“ Used for designing, managing, and querying the database schema  
- **venv** â€“ Virtual environment for dependency management (excluded via `.gitignore`)  

## ğŸ”„ ETL Workflow  
- **Extract** â€“ Retrieve raw data from AWS S3.  
- **Transform** â€“ Clean, preprocess, and structure data using Python and Pandas.  
  - Missing numeric values were filled with the median of the respective column.  
  - Missing categorical values were filled with the most common data (mode) of the respective column.  
- **Load** â€“ Insert processed data into AWS RDS using SQL.  

## ğŸ“Œ SQL for Structured Data Management  
SQL is a critical component of this project, enabling structured data management and analysis:  
- **Schema Definition** â€“ SQL scripts create the database schema, ensuring data is organized and accessible.  
- **Data Loading** â€“ SQL `INSERT` statements load the processed data into AWS RDS.  
- **Querying Data** â€“ SQL queries analyze the data, revealing insights and trends.  
- **Database Management** â€“ MySQL Workbench was used to design and manage the relational database schema.  

## ğŸ¯ Key Features  
âœ”ï¸ **Data Retrieval** â€“ Fetch raw data from AWS S3 using Boto3.  
âœ”ï¸ **Data Transformation** â€“ Clean and preprocess data using Python and Pandas, including handling missing values.  
âœ”ï¸ **Data Loading** â€“ Use SQL to create tables and insert processed data into AWS RDS.  
âœ”ï¸ **Scalability** â€“ Designed for scalability and future improvements.  
âœ”ï¸ **Virtual Environment Management** â€“ Used `venv` for dependency isolation, with virtual environment folders excluded from version control (`.gitignore`).  
âœ”ï¸ **Database Visualization & Management** â€“ Utilized MySQL Workbench for database schema visualization and management.  

## ğŸ“Œ Future Enhancements  
ğŸ”¹ **Advanced SQL Queries** â€“ Implement complex SQL queries for deeper analysis.  
ğŸ”¹ **Automated Reporting** â€“ Generate automated reports and dashboards using SQL.  
ğŸ”¹ **Optimization** â€“ Optimize SQL queries and database indexing for better performance.  




