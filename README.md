# AWS Data Pipeline: ETL Process from S3 to RDS  

## ğŸš€ Overview  
This project demonstrates an **ETL (Extract, Transform, Load) pipeline** using **AWS S3 and RDS** to move and process structured data. It focuses on extracting raw data from S3, transforming it using Python, and storing it in a relational database (RDS) for further analysis.  

## ğŸ”¹ Technologies Used  
- **AWS S3** â€“ Cloud storage for raw data  
- **AWS RDS (MySQL)** â€“ Relational database for structured data storage  
- **Python & SQL** â€“ Data transformation and loading  
- **Pandas** â€“ Data manipulation and preprocessing  
- **Boto3** â€“ AWS SDK for interacting with S3 and RDS  

## ğŸ”„ ETL Workflow  
1. **Extract** â€“ Retrieve raw data from AWS S3  
2. **Transform** â€“ Clean and format data using Python and Pandas  
3. **Load** â€“ Insert processed data into AWS RDS  

## ğŸ¯ Key Features
- âœ”ï¸ Data retrieval from AWS S3
- âœ”ï¸ Processing and cleaning data with Python
- âœ”ï¸ Storing structured data in AWS RDS
- âœ”ï¸ Designed for scalability and future improvements

## ğŸ“Œ Future Enhancements
- ğŸ”¹ Advanced SQL Queries for Analysis
