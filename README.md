# AWS Data Pipeline: ETL Process from S3 to RDS  

## 🚀 Overview  
This project demonstrates an **ETL (Extract, Transform, Load) pipeline** using **AWS S3 and RDS** to move and process structured data. It focuses on extracting raw data from S3, transforming it using Python, and storing it in a relational database (RDS) for further analysis.  

## 🔹 Technologies Used  
- **AWS S3** – Cloud storage for raw data  
- **AWS RDS (MySQL)** – Relational database for structured data storage  
- **Python & SQL** – Data transformation and loading  
- **Pandas** – Data manipulation and preprocessing  
- **Boto3** – AWS SDK for interacting with S3 and RDS  

## 🔄 ETL Workflow  
1. **Extract** – Retrieve raw data from AWS S3  
2. **Transform** – Clean and format data using Python and Pandas  
3. **Load** – Insert processed data into AWS RDS  

## 🎯 Key Features
- ✔️ Data retrieval from AWS S3
- ✔️ Processing and cleaning data with Python
- ✔️ Storing structured data in AWS RDS
- ✔️ Designed for scalability and future improvements

## 📌 Future Enhancements
- 🔹 Advanced SQL Queries for Analysis
